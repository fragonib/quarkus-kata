= Questions

* *Imagine that a second vendor is publishing data of the same nature (i.e., containing a timestamp, energy values and some type of ID, but perhaps with a different structure)*

.. *What strategy would you follow to accept this new format*

.. *Potentially, each vendor will send data in a different way.
How could we design the service/system so that we can include a new schema easily and with little or no downtime when adding it?*

+
In this particular solution the strategy adopted to deal with both requirements has been to decouple a _energy consumption report_ (coming from a particular probe) using a generic report.

+
The delivery infrastructure (specifically the REST endpoint listening for  that particular probe reports) has the mission of converting a _particular report_ into the _generic report_.

+
Beyond that point in code that _generic report_ is the only one that is known by the rest of the platform. There will be an endpoint per probe or manufacturer with this translation goal.

+

* *In case we would like to stream the processed data via Kafka, what would be a nice strategy to follow?
(Serialisation, message structureâ€¦)*

+
One convenient approach would be to use _Schema Registry_ together with _Kafka_. We should generate and register an AVRO schema (or any schema language supported by _Schema Registry_) that corresponds to the message to be produced (read log)

+
[source,json]
----
{
  "name": "DeviceEnergyReport",
  "type": "record",
  "namespace": "com.bia.charger.sensor",
  "fields": [
    {
      "name": "id",
      "type": "string"
    },
    {
      "name": "timestamp",
      "type": "int",
      "logicalType": "date"
    },
    {
      "name": "energyCounter",
      "type": "int"
    },
    {
      "name": "power",
      "type": {
        "name": "power",
        "type": "record",
        "fields": [
          {
            "name": "watts",
            "type": "int"
          },
          {
            "name": "timeUnit",
            "type": "string"
          }
        ]
      }
    },
    {
      "name": "deviceId",
      "type": "string"
    }
  ]
}
----

+
Regarding _message structure_ it's also convenient to add headers onto Kafka records to trace a transaction journey through the system (ie. B3 headers). We can use Quarkus extensions like: `quarkus-smallrye-opentracing`, `quarkus-opentelemetry-exporter-otlp`. Then we can use graphics to explore insights of application and take business decisions
